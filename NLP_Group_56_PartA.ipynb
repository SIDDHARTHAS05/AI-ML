{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203b13b3-6542-44e5-988d-04b6fbd71078",
   "metadata": {},
   "source": [
    "<b>\"PART – A \"</b>\n",
    "\n",
    "<b>Task 1: Grammar Error Correction Application </b>\n",
    "\n",
    "Objective:\n",
    "Develop a grammar error correction application that can detect and correct grammatical errors in written text. \n",
    "This system should be capable of handling a wide range of common errors, including but not limited to sentence structure mistakes, \n",
    "subject-verb agreement errors, punctuation issues, and incorrect word usage.\n",
    "The application should include both a web-based interface and backend processing using Flask and Python libraries.\n",
    "\n",
    "Requirements:\n",
    "Web Interface (3 Marks)\n",
    "Front-end Development:\n",
    "•\tCreate a web-based front-end using HTML and JavaScript.\n",
    "•\tProvide a text input area for users to enter the text they wish to have corrected, along with a file upload option that enables them to upload a text file for batch correction.\n",
    "User Input:\n",
    "•\tDisplay the corrected word immediately when manually entered.\n",
    "•\tCorrect grammar errors in the uploaded text file and generate a corrected version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711438b-867b-4136-b0f2-ccd924e26599",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '├' (U+251C) (3397285747.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ├── app.py                   # Flask backend\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '├' (U+251C)\n"
     ]
    }
   ],
   "source": [
    "# Solutions apporach:\n",
    "grammar_error_correction/\n",
    "├── app.py                   # Flask backend\n",
    "├── templates/\n",
    "│   └── index.html           # Frontend HTML\n",
    "├── static/\n",
    "│   ├── script.js            # JavaScript for frontend\n",
    "│   └── styles.css           # CSS for styling\n",
    "├── requirements.txt         # Python dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe07ab5-1342-46e6-aca7-d3722c9fd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. app.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import language_tool_python\n",
    "import os\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "app.config[\"UPLOAD_FOLDER\"] = \"uploads\"\n",
    "\n",
    "# Initialize LanguageTool\n",
    "tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "\n",
    "def correct_grammar(text):\n",
    "    \"\"\"Correct grammar using LanguageTool.\"\"\"\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    \"\"\"Render the homepage.\"\"\"\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/correct\", methods=[\"POST\"])\n",
    "def correct_text():\n",
    "    \"\"\"Correct a single text input.\"\"\"\n",
    "    data = request.json\n",
    "    input_text = data.get(\"text\", \"\")\n",
    "    corrected_text = correct_grammar(input_text)\n",
    "    return jsonify({\"corrected_text\": corrected_text})\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_file():\n",
    "    \"\"\"Handle file upload for batch grammar correction.\"\"\"\n",
    "    file = request.files[\"file\"]\n",
    "    if file and file.filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], file.filename)\n",
    "        file.save(file_path)\n",
    "\n",
    "        # Read file content and correct grammar line by line\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        corrected_lines = [correct_grammar(line.strip()) for line in lines]\n",
    "\n",
    "        # Save corrected file\n",
    "        corrected_file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], \"corrected_\" + file.filename)\n",
    "        with open(corrected_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(corrected_lines))\n",
    "\n",
    "        return jsonify({\"corrected_file\": corrected_file_path})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Invalid file type. Please upload a .txt file.\"}), 400\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(app.config[\"UPLOAD_FOLDER\"]):\n",
    "        os.makedirs(app.config[\"UPLOAD_FOLDER\"])\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731373a-ef87-4fae-abe2-f906bd7197df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63d695-56ba-4dc0-b372-f78429f72f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. templates/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Grammar Correction</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/styles.css\">\n",
    "    <script src=\"/static/script.js\" defer></script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Grammar Correction Application</h1>\n",
    "\n",
    "    <div class=\"manual-correction\">\n",
    "        <h2>Manual Text Correction</h2>\n",
    "        <textarea id=\"inputText\" placeholder=\"Type your text here...\"></textarea><br>\n",
    "        <button onclick=\"correctText()\">Correct Grammar</button>\n",
    "        <h3>Corrected Text:</h3>\n",
    "        <p id=\"outputText\">The corrected text will appear here.</p>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"batch-correction\">\n",
    "        <h2>Batch File Correction</h2>\n",
    "        <form id=\"fileForm\" enctype=\"multipart/form-data\">\n",
    "            <input type=\"file\" id=\"fileInput\" name=\"file\" accept=\".txt\"><br>\n",
    "            <button type=\"button\" onclick=\"uploadFile()\">Upload and Correct</button>\n",
    "        </form>\n",
    "        <h3>Corrected File:</h3>\n",
    "        <p id=\"fileOutput\"></p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6328e45-bab4-4342-99c1-955c60b79b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. static/script.js\n",
    "\n",
    "async function correctText() {\n",
    "    const inputText = document.getElementById(\"inputText\").value;\n",
    "\n",
    "    // Send user input to the backend\n",
    "    const response = await fetch(\"/correct\", {\n",
    "        method: \"POST\",\n",
    "        headers: { \"Content-Type\": \"application/json\" },\n",
    "        body: JSON.stringify({ text: inputText }),\n",
    "    });\n",
    "\n",
    "    // Display the corrected text\n",
    "    const data = await response.json();\n",
    "    document.getElementById(\"outputText\").innerText = data.corrected_text || \"No corrections made.\";\n",
    "}\n",
    "\n",
    "async function uploadFile() {\n",
    "    const fileInput = document.getElementById(\"fileInput\");\n",
    "    const formData = new FormData();\n",
    "    formData.append(\"file\", fileInput.files[0]);\n",
    "\n",
    "    // Send file to the backend\n",
    "    const response = await fetch(\"/upload\", {\n",
    "        method: \"POST\",\n",
    "        body: formData,\n",
    "    });\n",
    "\n",
    "    // Display the link to the corrected file\n",
    "    const data = await response.json();\n",
    "    if (data.corrected_file) {\n",
    "        document.getElementById(\"fileOutput\").innerHTML = `File corrected successfully! Download <a href=\"/${data.corrected_file}\" target=\"_blank\">here</a>.`;\n",
    "    } else {\n",
    "        document.getElementById(\"fileOutput\").innerText = data.error || \"Error processing the file.\";\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe24fb-e68c-4379-94b1-b38c6ba7d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Static/style.css\n",
    "\n",
    "body {\n",
    "    font-family: Arial, sans-serif;\n",
    "    margin: 20px;\n",
    "    padding: 20px;\n",
    "    background-color: #f9f9f9;\n",
    "}\n",
    "\n",
    "h1, h2, h3 {\n",
    "    color: #333;\n",
    "}\n",
    "\n",
    "textarea {\n",
    "    width: 100%;\n",
    "    height: 100px;\n",
    "    margin-bottom: 10px;\n",
    "    padding: 10px;\n",
    "    font-size: 14px;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 5px;\n",
    "}\n",
    "\n",
    "button {\n",
    "    padding: 10px 20px;\n",
    "    font-size: 14px;\n",
    "    background-color: #007BFF;\n",
    "    color: #fff;\n",
    "    border: none;\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "button:hover {\n",
    "    background-color: #0056b3;\n",
    "}\n",
    "\n",
    ".manual-correction, .batch-correction {\n",
    "    margin-top: 20px;\n",
    "    padding: 20px;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 10px;\n",
    "    background-color: #fff;\n",
    "}\n",
    "\n",
    "p {\n",
    "    font-size: 14px;\n",
    "    color: #555;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103ee8-1d36-49ac-9ffb-612d5718c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Flask\n",
    "language-tool-python\n",
    "Werkzeug\n",
    "nltk\n",
    "pandas\n",
    "matplotlib\n",
    "numpy\n",
    "transformers\n",
    "datasets\n",
    "# evaluate rouge score\n",
    "rouge\n",
    "torch\n",
    "pytorch-lightning\n",
    "datasets \n",
    "tqdm \n",
    "pandas\n",
    "sentencepiece\n",
    "transformers\n",
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd595f10-6ec2-4a27-87c4-dc339abe8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process flow: Create a workspace in VS with all folder structure as mentioned above and \n",
    "\n",
    "#     1. run app.py\n",
    "#     2. http://127.0.0.1:5000/\n",
    "#     3. Test the application:\n",
    "\n",
    "# Enter text for manual correction.\n",
    "# Upload a .txt file for batch correction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d15c55-0dcc-4527-bc32-6c61199736af",
   "metadata": {},
   "source": [
    "<b>Grammar Error Correction Application (3 Marks)</b>\n",
    "\n",
    "Backend Implementation:\n",
    "\n",
    "Use Flask to develop the backend for handling user requests and responses.\n",
    "Use any Python library, to create grammar error correction with given text as corpus.\n",
    "Use text from below source as corpus for calculating probabilities.\n",
    "Use https://www.kaggle.com/datasets/dariocioni/c4200mLinks to an external site. dataset. ( This dataset is huge so use 1/4th of the dataset).\n",
    "Integration: (2 Marks)\n",
    "\n",
    "Integrate the front-end and back-end components to ensure seamless functionality.\n",
    "Process user input effectively, perform necessary corrections, and present results in a clear and user-friendly manner on the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c01b70-3762-403d-8ecf-836f7d7adcc8",
   "metadata": {},
   "source": [
    "<b>Integration: (2 Marks)</b>\n",
    "\n",
    "Integrate the front-end and back-end components to ensure seamless functionality.\n",
    "Process user input effectively, perform necessary corrections, and present results in a clear and user-friendly manner on the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5c2bb-656d-408d-955c-c6c10f59546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_error_correction/\n",
    "├── preprocess_dataset.py      # Preprocess dataset (create incorrect-correct pairs)\n",
    "├── train_model.py             # Fine-tune T5 model\n",
    "├── evaluate_model.py          # Evaluate model performance\n",
    "├── app.py                     # Flask backend\n",
    "├── templates/\n",
    "│   └── index.html             # Frontend HTML\n",
    "├── static/\n",
    "│   ├── script.js              # JavaScript\n",
    "│   └── styles.css             # CSS for styling\n",
    "├── data/\n",
    "│   ├── C4_200M_sampled.csv    # Sampled dataset\n",
    "│   ├── grammar_dataset.csv    # Preprocessed dataset\n",
    "│   └── corrected_file.txt     # Corrected text file (generated after file upload)\n",
    "├── models/\n",
    "│   └── grammar_model/         # Fine-tuned T5 model\n",
    "├── requirements.txt           # Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f90d6f-04dd-4233-b7cf-6cc7630ebdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocessing the data\n",
    "# preprocess_dataset.py\n",
    "import dask.dataframe as dd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import  tqdm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Example loop with a progress bar\n",
    "for i in tqdm(range(100), desc=\"Processing Items\"):\n",
    "    time.sleep(0.1)  # Simulating work\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# File paths\n",
    "file_path = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\C4_200M.tsv-00000-of-00010\"  # Full dataset file\n",
    "output_sample_file = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\C4_200M_sampled.csv\"\n",
    "output_tokenized_file = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\tokenized_corpus.txt\"\n",
    "\n",
    "# Define the fraction for sampling (1/4th of the dataset)\n",
    "sampling_fraction = 0.01\n",
    "\n",
    "# Load dataset with Dask\n",
    "print(\"Loading dataset with Dask...\")\n",
    "ddf = dd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# Sample 1/4th of the dataset\n",
    "print(\"Sampling 1/4th of the dataset...\")\n",
    "sampled_ddf = ddf.sample(frac=sampling_fraction, random_state=42)\n",
    "\n",
    "# Save sampled data to a CSV\n",
    "print(f\"Saving sampled dataset to '{output_sample_file}'...\")\n",
    "sampled_ddf = sampled_ddf.compute()  # Convert Dask DataFrame to Pandas DataFrame\n",
    "sampled_ddf.columns = [\"id\", \"text\"]  # Assign column names if necessary\n",
    "sampled_ddf.to_csv(output_sample_file, index=False)\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs and special characters\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "print(\"Tokenizing the sampled dataset...\")\n",
    "tokenized_corpus = []\n",
    "for text in sampled_ddf[\"text\"].dropna().astype(str):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    tokens = word_tokenize(preprocessed_text.lower())\n",
    "    tokenized_corpus.extend(tokens)\n",
    "\n",
    "# Save the tokenized corpus to a file\n",
    "with open(output_tokenized_file, \"w\") as f:\n",
    "    f.write(\" \".join(tokenized_corpus))\n",
    "print(f\"Tokenized corpus saved to '{output_tokenized_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef60cc-ec95-4071-965e-aa3decd8df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\C4_200M_sampled.csv\"  # Input sampled dataset\n",
    "output_file = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\grammar_dataset.csv\"  # Preprocessed dataset for training\n",
    "\n",
    "# Function to introduce synthetic grammar errors\n",
    "def introduce_errors(sentence):\n",
    "    errors = {\n",
    "        \"is\": \"are\",\n",
    "        \"was\": \"were\",\n",
    "        \"he\": \"him\",\n",
    "        \"she\": \"her\",\n",
    "        \"their\": \"there\",\n",
    "        \"your\": \"you're\",\n",
    "        \"has\": \"have\",\n",
    "    }\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in errors and random.random() < 0.3:  # 30% chance of replacing\n",
    "            words[i] = errors[words[i]]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(input_file)\n",
    "data = data.dropna(subset=[\"text\"])  # Ensure no missing text\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Create incorrect-correct pairs\n",
    "print(\"Generating incorrect-correct pairs...\")\n",
    "pairs = []\n",
    "for text in data[\"text\"]:\n",
    "    correct_sentence = text.strip()\n",
    "    incorrect_sentence = introduce_errors(correct_sentence)\n",
    "    if incorrect_sentence != correct_sentence:\n",
    "        pairs.append({\"incorrect\": incorrect_sentence, \"correct\": correct_sentence})\n",
    "\n",
    "# Save grammar dataset\n",
    "print(f\"Saving grammar dataset to '{output_file}'...\")\n",
    "grammar_dataset = pd.DataFrame(pairs)\n",
    "grammar_dataset.to_csv(output_file, index=False)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34cf9a-2935-4491-9fdb-a89ba29c4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "# Dataset class\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=128):\n",
    "        self.data = pd.read_csv(file_path).sample(1000)\n",
    "        self.data = self.data.dropna(subset=[\"incorrect\", \"correct\"])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        incorrect = str(self.data.iloc[idx][\"incorrect\"])\n",
    "        correct = str(self.data.iloc[idx][\"correct\"])\n",
    "\n",
    "        input_tokens = self.tokenizer.encode_plus(\n",
    "            incorrect,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_tokens = self.tokenizer.encode_plus(\n",
    "            correct,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_tokens[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_tokens[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_tokens[\"input_ids\"].squeeze(),\n",
    "        }\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "file_path = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\grammar_dataset.csv\"\n",
    "dataset = GrammarDataset(file_path, tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Training configuration\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Total Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_dir = \"models/grammar_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "print(f\"Model saved to '{model_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887f560-57e2-4695-aa76-2caedaa42b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e4de9-7ee7-4d5d-8e96-ea46cfc4e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3634ce2d-6390-4ed5-a77a-82ddaa85436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus BLEU Score: 0.9089\n",
      "Average ROUGE-1: 0.9459\n",
      "Average ROUGE-2: 0.8896\n",
      "Average ROUGE-L: 0.9459\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\VS-Code 1\\grammar_error_correction\\data\\grammar_dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize scorers\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# BLEU scores\n",
    "references = [[row[\"correct\"].split()] for _, row in data.iterrows()]\n",
    "candidates = [row[\"incorrect\"].split() for _, row in data.iterrows()]\n",
    "bleu_score = corpus_bleu(references, candidates)\n",
    "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# ROUGE scores\n",
    "rouge1, rouge2, rougeL = 0, 0, 0\n",
    "for _, row in data.iterrows():\n",
    "    correct = row[\"correct\"]\n",
    "    incorrect = row[\"incorrect\"]\n",
    "    scores = scorer.score(correct, incorrect)\n",
    "    rouge1 += scores[\"rouge1\"].fmeasure\n",
    "    rouge2 += scores[\"rouge2\"].fmeasure\n",
    "    rougeL += scores[\"rougeL\"].fmeasure\n",
    "\n",
    "n = len(data)\n",
    "print(f\"Average ROUGE-1: {rouge1 / n:.4f}\")\n",
    "print(f\"Average ROUGE-2: {rouge2 / n:.4f}\")\n",
    "print(f\"Average ROUGE-L: {rougeL / n:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3c6f3-8307-46fe-9733-ac7af2398fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f066f95a-112d-43ba-a1da-699e5a9893e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      " * Restarting with watchdog (windowsapi)\n",
      "INFO:werkzeug: * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sddhr\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_name = r\"D:\\BITS Pilani Sem 3\\NLP Applications\\Grammar_Err\\models\\grammar_model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "app.config[\"UPLOAD_FOLDER\"] = \"data\"\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    \"\"\"Serve the main webpage.\"\"\"\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/correct\", methods=[\"POST\"])\n",
    "def correct_text():\n",
    "    \"\"\"Corrects a single text string.\"\"\"\n",
    "    data = request.json\n",
    "    input_text = data.get(\"text\", \"\")\n",
    "\n",
    "    # Tokenize input and generate corrected text\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "    outputs = model.generate(input_tokens, max_length=128, num_beams=4, early_stopping=True)\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return jsonify({\"corrected_text\": corrected_text})\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_file():\n",
    "    \"\"\"Handles file uploads for batch grammar correction.\"\"\"\n",
    "    file = request.files[\"file\"]\n",
    "    if file and file.filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], file.filename)\n",
    "        file.save(file_path)\n",
    "\n",
    "        # Read the file and correct line by line\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        corrected_lines = []\n",
    "        for line in lines:\n",
    "            input_tokens = tokenizer.encode(line.strip(), return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "            outputs = model.generate(input_tokens, max_length=128, num_beams=4, early_stopping=True)\n",
    "            corrected_line = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            corrected_lines.append(corrected_line)\n",
    "\n",
    "        # Save corrected lines to a new file\n",
    "        corrected_file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], \"corrected_\" + file.filename)\n",
    "        with open(corrected_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(corrected_lines))\n",
    "        \n",
    "        return jsonify({\"corrected_file\": corrected_file_path})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Invalid file type. Please upload a .txt file.\"}), 400\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0279b-1c2f-43be-b3fd-a0a9d9cc625b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
